<?xml version="1.0" encoding="UTF-8"?><TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink"><teiHeader xml:lang="en"><fileDesc><titleStmt><title level="a" type="main">Pre-trained Language Model for Web-scale Retrieval in Baidu Search</title></titleStmt><publicationStmt><publisher/><availability status="unknown"><licence/></availability><date type="published" when="2021-06-07">7 Jun 2021</date></publicationStmt><sourceDesc><biblStruct><analytic><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName><email>liuyiding.tanh@gmail.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixue</forename><surname>Lu</surname></persName><email>luweixue@baidu.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suqi</forename><surname>Cheng</surname></persName><email>chengsuqi@gmail.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiting</forename><surname>Shi</surname></persName><email>shidaiting01@baidu.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName><email>wangshuaiqiang@baidu.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicong</forename><surname>Cheng</surname></persName><email>chengzhicong01@baidu.com</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName><email>yindawei@acm.org</email><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation><affiliation key="aff0"><orgName type="institution">Baidu Inc</orgName><address><settlement>Beijing</settlement><country key="CN">China</country></address></affiliation></author><title level="a" type="main">Pre-trained Language Model for Web-scale Retrieval in Baidu Search</title></analytic><monogr><imprint><date type="published" when="2021-06-07">7 Jun 2021</date></imprint></monogr><idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno><idno type="arXiv">arXiv:2106.03373v1[cs.IR]</idno></biblStruct></sourceDesc></fileDesc><encodingDesc><appInfo><application version="0.6.1" ident="GROBID" when="2021-06-09T07:42+0000"><desc>GROBID - A machine learning software for extracting information from scholarly documents</desc><ref target="https://github.com/kermitt2/grobid"/></application></appInfo></encodingDesc><profileDesc><textClass><keywords><term>\u2022 Information systems \u2192 Information retrieval</term><term>Retrieval models and ranking</term><term>Language models</term><term>Pretrained Language Models</term><term>Information Retrieval</term><term>Search</term></keywords></textClass><abstract><p>Retrieval is a crucial stage in web search that identifies a small set of query-relevant candidates from a billion-scale corpus. Discovering more semantically-related candidates in the retrieval stage is very promising to expose more high-quality results to the end users. However, it still remains non-trivial challenges of building and deploying effective retrieval models for semantic matching in real search engine. In this paper, we describe the retrieval system that we developed and deployed in Baidu Search. The system exploits the recent state-of-the-art Chinese pretrained language model, namely Enhanced Representation through kNowledge IntEgration (ERNIE), which facilitates the system with expressive semantic matching. In particular, we developed an ERNIE-based retrieval model, which is equipped with 1) expressive Transformer-based semantic encoders, and 2) a comprehensive multi-stage training paradigm. More importantly, we present a practical system workflow for deploying the model in web-scale retrieval. Eventually, the system is fully deployed into production, where rigorous offline and online experiments were conducted. The results show that the system can perform high-quality candidate retrieval, especially for those tail queries with uncommon demands. Overall, the new retrieval system facilitated by pretrained language model (i.e., ERNIE) can largely improve the usability and applicability of our search engine.</p></abstract></profileDesc></teiHeader><text xml:lang="en"><body><div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Search engines (e.g., Google, Baidu, Bing) are critical tools for people to find useful information from massive web documents. A modern search engine usually employ a multi-stage pipeline that gradually narrows down the number of relevant documents (i.e., web pages),</p><p>where retrieval is usually known as the very first stage. It aims at identifying a few hundreds or thousands of relevant candidates from the entire billion-scale corpus, which has significant impact to the overall capability of a search engine.</p><p>Nevertheless, the unprecedented scale and diversity of web documents impose many challenges to the retrieval system. First (C1), semantic matching <ref type="bibr" target="#b19">[20]</ref> is one of the most critical concern, while conventional methods based on text matching (e.g., BM25 <ref type="bibr" target="#b33">[34]</ref>) may easily fail at modeling relevant information with different phrasing. Worse still, an increasing number of queries are in the style of natural language, making the semantic modeling even more challenging. Second (C2), both search queries and web contents are highly heterogeneous, following long-tail distributions. For example, a large amount of low-frequency queries (i.e., tail queries) have never been seen before by the search engine. As such, the semantics of tail queries and documents are difficult to be accurately inferred. Third (C3), to create significant impact to real-world applications, it also calls for practical solutions on deploying the retrieval model to serve web-scale data.</p><p>Extensive efforts from both academia and industry have been dedicated to tackle these challenges. To conduct semantic retrieval, a wealth of studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> have explored various bi-encoder models (i.e., Siamese networks or two-tower models), where different representation learning techniques has been employed as semantic encoders. Given a query and a document, the semantic encoder takes the query text and the document text (e.g., title) as inputs, and respectively produces two embeddings for the relevance computation. Recently, BERT <ref type="bibr" target="#b3">[4]</ref> has made significant progress on natural language understanding, and thus has also been applied as a more powerful semantic encoder <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. The BERT-based bi-encoder and its variants have achieved the state-of-the-art performance on retrieval tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>, which can be mainly attributed to the expressive deep attention-based structure (i.e., Transformer) and the pretraining and fine-tuning paradigm. Although considerable research progress has been made, there is still a lack of investigation on how to develop and deploy such retrieval models in the online environment of a search engine.</p><p>Present work. In this paper, we introduce a novel retrieval system that is fully deployed in Baidu Search <ref type="bibr" target="#b0">1</ref> , the dominant search engine in China. In particular, the retrieval system is equipped with the recent state-of-the-art Chinese pretrained language model (PLM), namely Enhanced Representation through kNowledge IntEgration</p><p>\u2022 To tackle the first challenge (C1), we leverage an ERNIE-based (i.e., Transformer-based) bi-encoder to perform expressive semantic matching. In particular, the Transformer encoders directly take the raw texts of queries and web documents as inputs and encode their semantics in latent embeddings. The deep structure of Transformer encoders allow the complicated semantics to be more comprehensively modeled. The dense attention over the raw texts can also keep the semantics of fine-grained context, such as using different prepositions (e.g., "for" vs. "to"), which is more friendly to conversational queries. Moreover, we integrate a poly-interaction scheme <ref type="bibr" target="#b15">[16]</ref> and effective training data mining strategies, which further improves the effectiveness of the retrieval model. \u2022 To tackle the second challenge (C2), we further propose a multistage training paradigm for optimizing the retrieval model. In particular, the training stages are designed with different data sources and objectives, which allows rich knowledge to be absorbed by the model. Compared with training-from-scratch, our proposed paradigm can further boost the generalization ability of the model, which is especially beneficial for tail queries. \u2022 To tackle the third challenge (C3), we develop a system architecture that serves the proposed model for large-scale web retrieval. In particular, we incorporate the semantic matching with conventional text matching to collaboratively generate relevant candidates. Moreover, we further introduce a lightweight postretrieval filtering module that provides an unified filtering for the retrieval, where more statistical features (e.g., clickthrough rate, dwell time) can be introduced to consider the overall quality. Overall, the system architecture allows the proposed model to work together with conventional text-matching workflow and offers flexibility for including other features.</p><p>To the best of our knowledge, this is one of the largest applications of pre-trained language models for web-scale retrieval. We anticipate this paper to provide our practical experiences and new insights for incorporating PLMs in web retrieval. The main contributions can be summarized as follows:</p><p>\u2022 The retrieval model. We leverage Transformers as the semantic encoders for web retrieval, and further exploit a poly-interaction scheme and several strategies for mining training data. The model can capture complicated semantic information underlying query and web documents, and is effective for semantic matching. \u2022 Training paradigm. We introduce a novel multi-stage training paradigm that facilitates the retrieval model to learn rich information. This would significantly improve the generalization ability of the model, especially for the tail queries. \u2022 System design. We design an effective and efficient system workflow to server the model, allowing it to seamlessly work with the conventional workflow to boost the overall performance. We also introduce a post-retrieval filtering module, which integrates statistical features and semantic relatedness that measure the overall quality of the candidates in an unified manner. \u2022 Evaluation. We conduct extensive offline and online experiments to validate the effectiveness of the retrieval system. The results show that the deployment of the system can significantly improve the usability and applicability of the search engine.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Semantic Retrieval in Web Search</head><p>Semantic retrieval is essential for a modern retrieval system. Typical structures of semantic retrieval models can be viewed as bi-encoders or Siamese networks <ref type="bibr" target="#b4">[5]</ref>, which comprises two encoders that conduct semantic modeling. Most of the existing studies mainly focus on designing the encoders with different representation learning techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>. A representative work, namely Deep Semantic Similarity Model (DSSM) <ref type="bibr" target="#b13">[14]</ref>, is one of the earliest DNN methods for semantic modeling with clickthrough data. The deep fully-connected architectures of the DSSM encoders can extract expressive semantic information and achieve superior performance on web search. Thereafter, other DNN-based retrieval methods have thrived over the past years <ref type="bibr" target="#b26">[27]</ref>, including those based on Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> and Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. The common bi-encoder structure of those models allows a large number of document embeddings to be pre-computed and cached offline, which is very efficient for online retrieval. Therefore, such model structure has also been developed in real-world products <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47]</ref>. For instance, Huang et al. <ref type="bibr" target="#b12">[13]</ref> employ DNNbased bi-encoder in Facebook Search system, and introduce various practical experiences in the end-to-end optimization of the system. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> introduce embedding learning for the semantic retrieval in their E-commerce search service. Besides, Yin et al. <ref type="bibr" target="#b45">[46]</ref> also adopts deep semantic matching in the core ranking module of Yahoo Search. Different from previous studies, we explore pretrained language model with multi-stage training to better perceive the semantics behind queries and documents.</p><p>Despite the above-mentioned bi-encoder models, interactionbased methods are also widely used in many information retrieval systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. As such, another line of research for semantic matching is to model query-document interaction with DNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>. However, they cannot cache the document embeddings offline, and thus are inefficient for retrieval. They are preferred for ranking stage, which will not be further discussed in this paper. In our search engine, interaction-based methods are exploited to build the PLM-based ranking system <ref type="bibr" target="#b52">[53]</ref>.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretrained Language Models</head><p>Pretrained Language Models (PLMs), such as ELMo <ref type="bibr" target="#b30">[31]</ref>, BERT <ref type="bibr" target="#b3">[4]</ref> and ERNIE <ref type="bibr" target="#b37">[38]</ref>, have achieved monumental success in natural language understanding. Notably, the recent state-of-the-art PLMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref> are usually based on Transformers <ref type="bibr" target="#b38">[39]</ref>, which exploit a deep structure with stacked multi-head attention and fully-connected layers. More importantly, they adopted unsupervised pretraining with large corpus, and thus can incorporate more useful knowledge in the models. As BERT and its successors (e.g., XLNet <ref type="bibr" target="#b42">[43]</ref>, RoBERTa <ref type="bibr" target="#b21">[22]</ref>) exhibit superior capacity in understanding textual data, a handful of studies start to leverage them for semantic retrieval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. For instance, Chang et al. <ref type="bibr" target="#b1">[2]</ref> propose an early attempt that introduces BERT-based bi-encoders for large-scale retrieval. It also studies the effects of several new pretraining tasks.</p><p>Humeau et al. <ref type="bibr" target="#b15">[16]</ref> advance such bi-encoder with attentive interaction for the query and document embeddings. Lu et al. <ref type="bibr" target="#b22">[23]</ref> explore the negative sampling strategies for BERT-based bi-encoders at both pretraining and finetuning stages. However, despite the initial success achieved by these work, there is still a lack of investigation on developing and deploying such model for web-scale retrieval, especially in real-world productions. In this paper, we propose to leverage the state-of-the-art Chinese pretrained language model, namely Enhanced Representation through kNowledge IntEgration (ERNIE) <ref type="bibr" target="#b37">[38]</ref>, for building the retrieval system of our search engine.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RETRIEVAL MODEL</head><p>In this section, we first present the task definition, and then introduce the details of our proposed retrieval model.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a query , the web retrieval task aims to return a set of relevant documents from a large corpus , where the size of the corpus can be tens of billions or more. We denote by + the set of documents relevant to the query. Different from conventional retrieval with term matching, semantic retrieval computes the relevance score (\u2022) in a learned embedding space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>, using similarity metrics (e.g., dot-product, cosine similarity) as:</p><formula xml:id="formula_0">( , ) = sim( ( ; ), ( ; )),<label>(1)</label></formula><p>where (or ) is the representation model (i.e., encoder) parameterized by (or ) that encodes the query (or document) to dense embeddings.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>In this work, we are interested in parameterizing the encoders and as deep Transformer models <ref type="bibr" target="#b38">[39]</ref> due to its expressive power in modeling natural language. The bi-encoder backbone. By implementing and as deep Transformers, we refer the paradigm described in Eq. (1) as biencoder <ref type="bibr" target="#b15">[16]</ref>. More specifically, let T be a multi-layer transformer encoder, which is a stack of transformer blocks. Each block consists of a multi-head self-attention (MHA) sublayer followed by a feed-forward (FFN) sublayer, where MHA allows the model to jointly attend to different subspaces and FFN aggregates the attention results of different heads. The encoder takes</p><formula xml:id="formula_1">{[CLS], 1 , \u2022 \u2022 \u2022 , } (or [CLS], \u2032 1 , \u2022 \u2022 \u2022 , \u2032 ), i.</formula><p>e., the tokenzied sequence of the raw query (or document) text, as the input and outputs an encoded <ref type="figure" target="#fig_0">Figure 1</ref>. Note that [CLS] is a pseudo token that aggregates information in the encoder for the subsequent matching. For each transformer layer, the parameters are shared between query and document encoders (i.e., and ), which has multiple potential benefits, such as reducing the number of model parameters so as to control model complexity <ref type="bibr" target="#b5">[6]</ref>, introducing prior knowledge to regularize models <ref type="bibr" target="#b40">[41]</ref>, and saving the storage space or memory size <ref type="bibr" target="#b16">[17]</ref>. The conventional bi-encoder computes the relevance score with a simple dot-product between the output CLS embeddings, i.e., ( , ) = \u2022 \u2032 . Poly attention. Different from vanilla bi-encoder, we further develop a poly attention scheme that enables more flexibility for modeling query semantics. This idea is originated from poly-encoder <ref type="bibr" target="#b15">[16]</ref> Prediction Phase  </p><formula xml:id="formula_2">sequence , 1 , \u2022 \u2022 \u2022 , (or \u2032 , \u2032 1 , \u2022 \u2022 \u2022 , \u2032 ) as described in</formula><formula xml:id="formula_3">... ! " # " $ !\u2032 ... " # &amp; ... "</formula><formula xml:id="formula_4">[CLS] 0 # 0 $ ... 1 !23 1 # 1 $ ... [CLS] 0 # &amp; 0 \' &amp; ... 1 !23 1 # &amp; 1 $ &amp; ... [CLS] 0 # 0 $ ... 1 !23 1 # 1 $ ... [CLS] 0 # &amp; 0 \' &amp; ... 1 !23 1 # &amp; 1 \' &amp; ...</formula><formula xml:id="formula_5">= ( ) 0 \u2022 + \u2211\ufe01 =1 ( ) \u2022 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">( ) 0 , \u2022 \u2022 \u2022 , ( ) = Softmax ( \u2022 , \u2022 \u2022 \u2022 , \u2022 ). These global context features 1 , \u2022 \u2022 \u2022 ,</formula><p>can be interpreted as different aggregations of the semantics from all the query terms, which reflects fine-grained query demands. Subsequently, each of the global context features is compared with the document representation \u2032 using dot-product, followed by a max-pooling to finalize the relevance score, i.e.,</p><formula xml:id="formula_7">( , ) = max =1 \u2022 \u2032 .<label>(3)</label></formula><p>In the retrieval/prediction phase, we employ a slightly different strategy due to a practical concern. In real application with massive web documents, document representations must be pre-computed to build indexes, which enable efficient retrieval with fast nearest neighbor search. In such case, relevance computation based on Eq.</p><p>(3) is clearly infeasible, as such metric with max-pooling france can hardly be supported by existing index schemes. Motivated by this, we propose to construct an unified surrogate embedding with mean pooling over all , i.e., 1 =1 . As such, the final relevance score is defined as</p><formula xml:id="formula_8">! " ! # ! $ ! % &amp; " \' &amp; # \' &amp; $ \' &amp; % \' &amp; " ( &amp; # ( &amp; $ ( &amp; % ( ! ) query &amp; ) \' relevant doc &amp; )<label>(</label></formula><formula xml:id="formula_9">( , ) = \u2022 \u2032 = 1 \u2211\ufe01 =1 \u2022 \u2032 .<label>(4)</label></formula><p>With such definition, index-based nearest neighbor search can be leveraged, which largely improves the applicability of the model. Remark. As already mentioned, the relevance score is calculated inconsistently between training and predicting phases. In practice, we find out that such inconsistency would not undermine the model performance for semantic retrieval. Therefore, through bagging distinct features learnt by the context codes, we can achieve a better and more heterogeneous representation of the query for a more powerful semantic matching with that of the document.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>We formulate the learning procedure of the retrieval model using maximum likelihood estimation: Given a query , all of its relevant documents + and irrelevant ones \u2212 , the optimal parameters * , * can be defined as:</p><formula xml:id="formula_10">* , * = arg max , \u2211\ufe01 \u2211\ufe01 + \u2208 + ( + | , + , \u2212 ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_11">( + | , + , \u2212 )</formula><p>is the probability that can separate a relevant document + from all irrelevant ones \u2212 . Note that we consider the context codes are the parameters of the query encoder, which are jointly trained. We implement ( + | , + , \u2212 ) as a contrastive probability, i.e.,</p><formula xml:id="formula_12">(</formula><formula xml:id="formula_13">+ | , + , \u2212 ) = exp ( , + )/ exp ( , + )/ + \u2211\ufe01 \u2212 \u2208 \u2212 exp ( ( , \u2212 )/ ) ,<label>(6)</label></formula><p>where , which is normally set to 1, is the temperature of the softmax operation <ref type="bibr" target="#b11">[12]</ref>. Using a higher value for produces a softer probability distribution over classes (i.e., relevant or irrelevant). Furthermore, as it is inapplicable to know and use all the real relevant and irrelevant documents in the training, we reformulate the optimization problem with sampled relevant and irrelevant documents (respectively denoted as\u02c6+ and\u02c6\u2212) as * , * = arg max , \u2211\ufe01 \u2211\ufe01</p><formula xml:id="formula_14">+ \u2208\u02c6+ ( + | , + ,\u02c6\u2212).<label>(7)</label></formula></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Data Mining</head><p>To solve the optimization problem as Eq. <ref type="formula" target="#formula_14">7</ref>, the key of model training lies on the construction of\u02c6+ and\u02c6\u2212, such that a better model can be learned for semantic retrieval. In this subsection, we elaborate how we mine the positive and negative data for training. Positives and negatives in different data sources. For mining positive and negative examples, we first consider two kinds of data sources that are commonly used in practice:</p><p>\u2022 Search log data, where the queries and documents are logged with click signal, i.e., whether the document is clicked by the user. For each query, we use those clicked results as positives and those exposed but non-click results as negatives, as clicks can roughly indicate the satisfaction of user\'s intent. \u2022 Manually labeled data is usually collected with fine-grained grades (i.e., 0 to 4) assigned by human evaluation. For each query, the positive and negative examples are defined in a pairwise manner. For a given document that is considered as positive, other lower-grade documents under the same query can be considered as negatives.</p><p>In-batch negative mining. The goal of online retrieval is to distinguish a tiny fraction of relevant documents from massive irrelevant documents (i.e., |</p><formula xml:id="formula_15">+ | \u226a | \u2212 | \u2248 | |)</formula><p>. Instead of logging (or labeling) many irrelevant documents, we leverage an in-batch negative mining scheme <ref type="bibr" target="#b17">[18]</ref>, which is a more efficient way to construct those completely irrelevant documents. To avoid confusion, we refer to the non-click (or lower-grade) samples as strong negatives, and the in-batch negatives as random negatives.</p><p>Particularly during the training of our model, we consider random negatives as those documents (i.e., positives and strong negatives) of other queries in the same mini-batch. As data are shuffled before fed to the model, the random negatives are generally quite distinct from the query, which helps mimic the online retrieval scenario, i.e., identifying positives from massive random negatives. In addition, to collect sufficient random negatives from each minibatch, we exploit a simple yet practical solution, i.e., increasing the batch size. We apply mix-precision method that allows larger batch size during training with a fixed memory consumption. The batch size is set to make maximum use of GPU memory. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of the overall negative sampling strategies in a more intuitive way. In the example, the mini-batch of size 4 consists of four triplets {( , + , \u2212 ), = 1, 2, 3, 4}. For the query 1 , the positive and strong negative documents are respectively denoted as + 1 and \u2212 1 , and</p><formula xml:id="formula_16">+ 2 , + 3 , + 4 , \u2212 2 , \u2212 3 , \u2212</formula><p>4 are random negatives. For each row (i.e., each query) in <ref type="figure" target="#fig_1">Figure 2</ref>, there is one positive and a set of negatives, which can be used as + and \u2212 in Eq. (7). Therefore, the training data mined with these strategies can be used to optimize the retrieval model based on Eq. (7).</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING PARADIGM</head><p>The training paradigm of pretraining and finetuning has been widely employed for model optimization in many natural language processing problems. Representations learned by such paradigm usually show competitive performance across many tasks. However, such superiority has not yet been fully exploited for web retrieval. In this section, we propose a novel multi-stage training paradigm. Particularly, as shown in <ref type="figure">Figure 3</ref>, we divide the entire training  </p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stage 1: Pretraining</head><p>In this stage, we follow the same pretraining process in <ref type="bibr" target="#b37">[38]</ref>  </p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stage 2: Post-pretraining</head><p>Search engine processes billions of various queries with diverse goals or intentions every day, such as medical advice, travel guides, latest news, etc. Pretraining only on document corpus might be limited to tackle all kinds of requests from users. In this stage, we transfer previous pretrained model and continue pretaining on both query and document corpus of Baidu Search with the same tasks (i.e., MLM and NSP) as Stage 1. We name Stage 2 as post-pretaining, which keeps the model structure (i.e., a -layer Transformer) and the training tasks while changes the training data. Specifically, we collect one-month (i.e., tens of billions of) user search logs for post-pretraining. The tokenized raw texts of the query and the title of the document are concatenated as the input during this stage. Then we apply the same masking strategies as ERNIE for MLM and take clicked documents as the next sentence of the query for NSP.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stage 3: Intermediate Fine-tuning</head><p>From this stage, we start to fine-tune the retrieval model. We first fine-tune the retrieval model on an intermediate task, before finetuning again on the target task of interest <ref type="bibr" target="#b31">[32]</ref>. In particular, we leverage the post-pretrained ERNIE produced by Stage 2, as the encoders of our retrieval model, which is subsequently optimized using the same training data (i.e., search logs) as Stage 2. The training objective is as presented in Eq. <ref type="formula" target="#formula_14">7</ref>, and we construct positives and negatives for training as introduced in Section 3.4. Using an intermediate objective that more closely resembles our target task leads to better and faster fine-tuning performance.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stage 4: Target Fine-tuning</head><p>Finally, we fine-tune our retrieval model produced by Stage 3 on a relatively small manually-labeled data, which we consider is the most accurate and unbiased data for learning the retrieval model. For the data collection, a set of queries are sampled from query logs, and a certain number of query-document pairs are labeled according to their relevance judged by human editors. A 0-4 grade is assigned to each query-document pair based on the degree of relevance. The training objective is as presented in Eq. <ref type="formula" target="#formula_14">7</ref>, and we construct positives and negatives for training as introduced in Section 3.4.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DEPLOYMENT</head><p>In this section, we first introduce the embedding compression and quantization, which saves the online cost for retrieval. Then, we present the general picture of how the retrieval model works in the retrieval system. For the detailed implementation of our deployed model, please refer to the Appendix.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Embedding Compression and Quantization</head><p>We apply compression and quantization for the output embeddings of the retrieval model. On the document side, it can significantly reduce the memory cost for caching the embeddings; On the query side, it saves the transmission cost for query embedding, and thus improves the system throughput. Compression. We jointly train a compression layer with the ERNIE encoders. The compression layer is implemented as a fully-connected layer that takes the output embeddings of the encoders (i.e., and  <ref type="figure">Figure 4</ref>: The overall workflow of the ERNIE-enhanced retrieval system. \u2032 ) as inputs, and produces lower-dimensional embeddings. This would largely save the memory cost, with very slight decrease w.r.t. accuracy. In practice, we reduce the size of output embeddings from 768 to 256, which improves the overall efficiency of transmission and storage by 3 times. Quantization. Quantization is a very promising technique for boosting the efficiency and scalability of neural networks <ref type="bibr" target="#b14">[15]</ref>. For deploying our model to efficiently serving massive search queries, we apply a commonly-used int8 quantization to the output embeddings, where the overall efficiency can further be boosted by 4 times. More details of the embedding quantization can be referred to the Appendix.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System Workflow</head><p>An important fact for building an effective retrieval system is that, neither explicit text matching nor embedding-based semantic matching can handle all the various kinds of queries. Instead, it is more promising to integrate the two types of retrieval methods together, to provide a better overall performance. Motivated by this, we developed a novel system workflow, which is depicted in <ref type="figure">Figure 4</ref>. Compared with conventional text matching, the system is upgraded with two modules:</p><p>\u2022 ERNIE-based retrieval. The system enables the ERNIE-based retrieval model to work in parallel with the conventional retrieval workflow (i.e., text matching), which offers high maintainability of the system. \u2022 ERNIE-enhanced post-retrieval filtering. We further introduce an unified post-retrieval filtering module. It takes both retrieved documents from text matching and ERNIE-based semantic matching, and conduct effective post-retrieval filtering for further discovering high-quality candidates.</p><p>We elaborate the system workflow with the two modules, which including both offline and online stages. Offline database and index construction. As shown in the left part of <ref type="figure">Figure 4</ref>, during the offline stage, we use the trained ERNIE encoder to compute the embeddings (with the compression and quantization) for all documents in the corpus, based on which we build 1) Approximate Nearest Neighbor (ANN) indexes, and 2) an embedding database for the above-mentioned two modules, respectively. In particular, the ANN indexes are deployed for the ERNIE-based retrieval stage, to enable efficient embedding-based candidate generation via fast nearest neighbor search. The embedding database is used for the post-retrieval filtering stage. It is a key-value database for efficient document embedding lookup with given document ids. Next, we introduce the online workflow and show how the database and indexes are used online. Online integrated retrieval. For the online retrieval, we integrate conventional retrieval and the novel ERNIE-based semantic retrieval. The conventional retrieval first processes the query text with several operations (e.g., word segmentation, stop-word filtering), and then conducts keyword/term matching with term indexes (i.e., inverted indexes) to retrieve a set of documents. In parallel, semantic retrieval first fetch the query embedding. After online embedding compression and quantization, the query embedding is applied for semantic retrieval with the ANN indexes. The results produced by different retrieval approaches are merged to form a candidate pool. Online post-retrieval filtering. After the integrated retrieval, we further introduce an online post-retrieval filtering stage, which further narrows down the scale of the candidates. To achieve this, the filtering module contains a lightweight ranking model (e.g., GBRank or RankSVM) that conducts an unified comparison for the retrieval candidates. Particularly for each candidate document, we fetch the statistical features (denoted as 1 , 2 , 3 , ... in <ref type="figure">Figure 4</ref>), such as click-through rate, from the feature database. Note that the semantic matching score is also applied in this stage as an important feature (denoted as ), where the scores computed in semantic retrieval can be reused. For those candidates retrieved by text matching, where the semantic matching scores are missing, we fetch their embeddings from the embedding database, and then compute the scores with the query embedding. As the candidate pool formed by the online retrieval stage is small compared to the entire corpus, the online computation of the semantic matching scores does not significantly increase the time cost. By doing this, we unify the comparison of all candidate documents with several statistical features and a semantic feature, where the semantic modeling capacity of our retrieval model can also benefit the documents dug by conventional retrieval paradigm.</p><p>We conduct an offline evaluation for the proposed retrieval model. The implementation details of the model and more ablation study can be found in the Appendix.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We evaluate the retrieval model on the datasets that are collected from the real production environment. Note all the data does not contain any user-related information for the privacy concern. Manually-labeled dataset (Manual). This dataset contains 6,423 queries and 147,341 query-document pairs. Each query has around 22 retrieved results on average. For each query, we collect documents from each stage of the search pipeline to ensure the diversity of the dataset. The dataset is labeled on our crowdsourcing platform, where a group of experts are required to assign an integer score varies from 0 to 4 to each query-document pair. The score represents whether the content of the document w.r.t. the query is off-topic (0), slightly relevant (1), relevant (2), useful (3) or vital <ref type="bibr" target="#b3">(4)</ref>. Automatic-annotated datasets (Auto). We collect another two search log datasets, namely Auto-Rand and Auto-Tail, which contains random queries and tail queries, respectively. Here, the tail queries are identified as those who have no more than 10 searches per week. For each query, we use our search engine to collect the top-10 results in the final ranking stage as the positive examples (i.e., the relevant documents), and consider the top-10 results of other queries as negative examples. After filtering noise and meaningless queries, we finalize the two datasets, where Auto-Rand contains 18,247 queries and 112,091 query-document pairs, and Auto-Tail contains 78,910 queries and 750,749 query-document pairs.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>Positive-Negative Ratio. We report the positive-negative ratio (PNR) on Manual dataset. For a given query and the associated documents , the PNR can be formally defined as</p><formula xml:id="formula_17">= , \u2208 1( &gt; ) \u2022 1( ( , ) &gt; ( , )) \u2032 , \u2032 \u2208 1( \u2032 &gt; \u2032 ) \u2022 1( ( , \u2032 ) &lt; ( , \u2032 ))</formula><p>, <ref type="bibr" target="#b7">(8)</ref> where 1(\u2022) is an indicator function, i.e., 1( &gt; ) = 1 if &gt; , and 0 otherwise. PNR measures the consistency between the manual labels and the model scores. We report the averaged PNR values over all queries in the experiments. Recall@10. We report Recall@10 on Auto-Rand and Auto-Tail datasets. The metric is defined as @10 = | \u2229\u02dc|/10, wher\u1ebd represents the top-10 retrieval w.r.t. using the retrieval model considering all the documents in a dataset as the corpus, and is the set of collected ground truth. We report the averaged Recall@10 values over all queries in the experiments.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Offline Experimental Results</head><p>In the offline evaluation, we report the experimental results of the proposed model during different training stages. Here, the performance of pretraining and post-pretraining are reported when using the -layer Transformer in a bi-encoder model. Besides, we also include a baseline method, i.e., the model that is used in the system before deploying the ERNIE-base retrieval model. <ref type="table" target="#tab_4">Table 1</ref> shows the results, where some key findings are observed: \u2022 The performance on different stages indicate that our proposed training paradigm is able to gradually improve the performance of the retrieval model. In particular, we can see that the model in the pretrain stage does not contain any domain knowledge w.r.t. the retrieval tasks, and thus performs poorly, where the Recall@10 is below 20% on both Auto datasets. However, after applying the multi-stage training paradigm, the Recall@10 of the model finally reaches 87.90% and 71.13% on Auto-Rand and Auto-Tail, respectively. \u2022 By applying the multi-stage training, the retrieval model can outperforms our online baseline w.r.t. Recall@10 on both Auto datasets. Moreover, we also observe that the relative improvement of our proposed model over the online base is large on Auto-Tail (\u0394 = 8.1%) than Auto-Rand (\u0394 = 3.1%), in terms of Recall@10. This shows that our proposed method can better discover relevant results for tail queries, which is very helpful in improving the user experience for the search engine. \u2022 In addition, we can see that the proposed model can beat the online baseline by a large margin w.r.t. PNR, where the value is improved from 1.77 to 2.48. This tells us that the proposed model not only can retrieval relevant documents, but also prefer high-quality results, i.e., the documents with higher manually-labeled grades.</p><p>Overall, our proposed model is able to gain superior performance on semantic retrieval through the multi-stage training, and beat the online baseline by a significant margin.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ONLINE EVALUATION</head><p>To investigate the impact of our proposed system to the search engine, we deploy the new system and conduct online experiments to compare it with the old retrieval system.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Interleaved Comparison</head><p>Interleaving <ref type="bibr" target="#b2">[3]</ref> is a commonly-used technique for evaluating industrial information retrieval systems (e.g., recommender systems, search engines). In interleaved comparison, the results of two systems are interleaved and exposed together to the end users, whose click behaviors would be credited to the system that provides the clicked results. The gain of the new system A over the base system B can be quantified with \u0394 , which is defined as</p><formula xml:id="formula_18">\u0394 = ( ) + 0.5 * ( , ) ( ) + ( ) + ( , ) \u2212 0.5,<label>(9)</label></formula><p>where ( ) (or ( )) is a counter that would be increased by 1 if the results produced by (or ) is preferred by the user, and ( , ) is increased by 1 otherwise. Intuitively, \u0394 &gt; 0  means is better than . To further reduce the bias in the evaluation, we further introduce a time-weighted version of \u0394 , denoted as \u0394 -tw, where the counters are weighted by the post-click dwell time (mapped to [0, 1] with a sigmoid function). As such, the \u0394 -tw can better reflect the relevance of the results with higher confidence, as users would stay in a web page longer if it is relevant. We conduct balanced interleaving experiments <ref type="bibr" target="#b2">[3]</ref> for comparing the ERNIE-enhanced system against the old retrieval system. The results are shown in <ref type="table" target="#tab_5">Table 2</ref>, which comprise the performance of different modules and for different types of queries.</p><p>\u2022 First, we observe \u0394 &gt; 0 and \u0394 -tw &gt; 0 in all the experiments, which indicate that the new system can increase user clicks with more relevant web documents.</p><p>\u2022 Second, the results also indicate that both the ERNIE-based retrieval and post-retrieval filtering can boost the effectiveness of the system, while the ERNIE-based retrieval generally achieves larger gain than the post-retrieval filtering, as it is effected before the filtering stage. \u2022 Third, we can see that the new system can achieve better performance on the queries with low search frequency, i.e., tail query. This validates that our proposed retrieval system has more significant improvement for the tail queries. \u2022 Also, the improvements on long queries (e.g., natural language queries) is larger than on short queries, which might indicate that the system can better handle the natural language queries.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Online A/B Test</head><p>We also conduct online A/B Test that compares the new system with the old system for one week. In the online A/B test, We mainly focus on the metrics that directly related to user experience. The results show that the proposed retrieval system can largely improve the overall user experience of the search engine. In particular, the number of click behaviors has increased by 0.31%. The number of click-and-stay behavior has increased by 0.57%. The average postclick dwell time has increased by 0.62%. The click-through rate has increased by 0.3%. All the reported values are statistically significant with &lt; 0.05. This shows that accurate semantic modeling and semantic matching in the retrieval stage is very helpful for improving the user engagement for the search engine.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Manual Evaluation for Online Cases</head><p>To more comprehensively show the impact of our proposed system, we further conduct a manual evaluation on the final ranking results with some real user-generated queries. This directly reflects the quality of the results exposed to the end users. Data preparation. We log a set of (several hundreds) queries and the corresponding final impressions, i.e., the top-ranked web documents in the final ranking stage, using the ERNIE-enhanced and the old retrieval systems. Note that the data logging is conducted by multiple rounds to eliminate randomness. We filter out whose queries that have identical impressions between the two systems, and then use the rest for the manual evaluation. The relative improvement validated by manual evaluation is given in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>Discounted Cumulative Gain (DCG). We first log a dataset and manually label the data with 0 to 4 grades, and then report the relative improvement w.r.t. the average Discounted Cumulative Gain (DCG) over the top-4 final results of all queries). The DCG is a widely-used metric and thus we omit its definition here. As shown in <ref type="figure">Figure 3</ref>, the results again show that our proposed system is able to improve the effectiveness of retrieval, especially for tail queries.</p><p>Side-by-side comparison. Besides, we also conduct a side-by-side comparison between the two systems. We log another dataset, and require the human experts to judge whether the new system or the base system gives better final results. Here, the relative gain is measured Good vs. Same vs. Bad (GSB) as</p><formula xml:id="formula_19">\u0394 = #Good \u2212 #Bad #Good + #Same + #Bad ,<label>(10)</label></formula><p>where #Good (or # Bad) indicates the number of queries that the new system provides better (or worse) final results. <ref type="table" target="#tab_6">Table 3</ref> shows that for both random queries and tail queries, the new ERNIE-enhanced system can significantly outperform the base system.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we described the novel retrieval system that is facilitated by pretrained language model (i.e., ERNIE). We developed and deployed the system in Baidu Search, which is highly effective in conducting semantic retrieval for web search. The system employs 1) an ERNIE-based retrieval model, 2) a multi-stage training paradigm and 3) a unified workflow for the retrieval system. Extensive offline and online experiments has shown that the retrieval system can significantly improve the effectiveness and general usability of the search engine.  </p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">APPENDIX 9.1 Implementation Details</head><p>Computation resources. We implement the proposed retrieval model with PaddlePaddle (version 1.6.2) <ref type="bibr" target="#b1">2</ref> , an open-source library for developing and deploying deep learning models. The model is pretrained and finetuned with 32 and 8 NVIDIA V100 GPUs (32GB Memory), respectively, on our distributed training platform. Parameter settings. We use 6-layer Transformers as the query and document encoders. In the input layer, we tokenize a given query or document title into Chinese characters as integer tokens, and map them into a set of embedding vectors with size 768. For each Transformer layer, we set the dimension of each hidden token representation as 768, the number of heads as 12, i.e., each head produces a 64 dimensional output. Besides, we set the number of context codes (i.e., ) as 16, and the dimension of the compression layers as 256. For model optimization, we use an Adam optimizer, and set the learning rate as 2e-5 and the batch size as 160 in all stages. For each training stage, we apply 4,000 warm up steps for the learning rate, and a 0.01 decay rate afterwards. During the training, we use a 0.1 dropout rate for all the layers to random drop the attention weights. Other unmentioned details are set as the same as vanilla ERNIE model.   Implementations. We present the pseudo code to depict the overall implementation of our method for the sake of reproducibility.</p><p>Note that the the in-batch random negative samples are implemented in train_interaction() by matrix multiplications, which is very efficient in practice. Such implementation is also adopted by other related work (e.g., https://github.com/chijames/Poly-Encoder).</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Details of Embedding Quantization</head><p>In particular, the output query and document embeddings are all quantized from float32 to uint8. According to output embeddings on a large-scale validation dataset, we calculate the data range ( , ) for each dimension of the output embeddings. Then, we divide the data range into = 255 equal intervals of Length</p><p>, where = ( \u2212 )/ . For the value in dimension of a given output embedding, when performing quantization, its quantized index ( ) is calculated by</p><formula xml:id="formula_20">( ) = \u230a( \u2212 )/ \u230b.</formula><p>This index is in range [0, 255], which can be representation as a 8-bit integer. When performing online inference, we recover a quantized value\u02dcby\u02dc= ( ) * + /2 + to approximate . This quantization only causes a small loss of precision which is ignorable for inference, but achieves significant improvements on development efficiency. The quantitative experimental results can be found in <ref type="table" target="#tab_7">Table 4</ref>. On the document side, this local quantization helps the system further save huge storage cost. On the query side, it significantly reduces the transmission cost, as the relevance computation for a given query would be distributed to multiple workers. Through the local quantization, the transmission and storage overheads further decrease to 1/4.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Offline Ablation Study</head><p>We further conduct several offline experiments to study the impact of some details of the retrieval model. For the experiments, we use two validation datasets, i.e., a search log dataset and a manuallylabeled dataset. The construction of the two types of datasets can be found in Section 3.4. The manually-labeled dataset is the same as we introduced in Section 6.1, and the search log data is isolated from the trillion-scale training data, which contains 300,000 query-document pairs. Note that all the following experiments are conducted for the model trained after intermediate finetuning (i.e., Stage 3).</p><p>9.3.1 Comparison with vanilla bi-encoder. Compared with vanilla ERNIE-based bi-encoder, our retrieval model is quite different, i.e., facilitated with poly attention mechanism, in-batch negative sampling strategy, and compression &amp; quantization. To clarify the influence of each of these differences, <ref type="table" target="#tab_7">Table 4</ref> shows the offline experiments of how these distinct features are layered up in our model. Note that the base (i.e., vanilla ERNIE-based bi-encoder) is optimized with hinge loss, where the margin is set to 0.1. We can see from the table that 1) the poly-attention and in-batch negatives (denoted as IBN) can significantly improve the model performance on both datasets, 2) the compression would largely reduce the storage cost, but slightly sacrifice the effectiveness, 3) the quantization is shown to be loss-free w.r.t. the PNR metric on the offline datasets, which is very promising to be adopted online.</p></div><div xmlns="http://www.tei-c.org/ns/1.0"><head>9.3.2</head><p>The training-prediction inconsistency. As mentioned in Section 3.2, we apply inconsistent schemes to finalize the output score during training and prediction. Here, we conduct experiments to investigate how the inconsistency would affect the performance of the model. <ref type="table" target="#tab_8">Table 5</ref> shows the PNR values of the model using different scoring methods on three extract validation datasets. Note that the model used here is an intermediate version, and thus the results might not be aligned with other experiments. In the table, the first row represents the scoring method used in training (i.e., Eq. (3)), the second row represents the scoring method used in prediction (i.e., Eq. (4)), and 0 to 4 indicate using fixed single global representation for the prediction, who are randomly sampled from the all 16 of them. We can see that our adopted mean-pooling method (i.e., the second row) performs similarly to the scoring method used in training. Thus, such inconsistency does not undermine the performance of the model during prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architectures and relevance computation.and is slightly customized in our retrieval model, which works differently during training and prediction phases.In the training phase, as shown in the upper half part ofFigure 1, we introduce a set of context codes, i.e., 0 , \u2022 \u2022 \u2022 , \u22121 , where each extracts a global representation by attending over all the outputs of the query encoder (i.e., , 1 , \u2022 \u2022 \u2022 , ) as</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of negative sampling during training.</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ERNIE 3 scoreFigure 3 :</head><label>33</label><figDesc>Training Paradigm of ERNIE-based Retrieval Model. process into four stages: (1) pretraining, (2) post-pretraining, (3) intermediate fine-tuning, and (4) target fine-tuning. Each stage is carefully designed with different training data and objectives, which boost the generalization ability of the model. The overall training pipeline is shown inFigure 3.</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>def model_encode_query(tokens): all_embeds = ERNIE_encoder.get_all_outputs(tokens) poly_embeds = poly_attention(all_embeds, context_codes) return [fc_compression(poly_embeds[i]) for i in range(m)] def model_encode_doc(tokens): cls_embed = ERNIE_encoder.get_cls_output(tokens) return fc_compression(cls_embed) def train_interaction(q, pos, neg): all_logits1, all_logits2 = [], [] for i in range(m): # in-batch random negative sampling via matrix multiplication pos_logits_with_rand_neg = matmul(q[i], pos.T) neg_logits_with_rand_neg = matmul(q[i], neg.T) all_logits1.append(pos_logits_with_rand_neg) all_logits2.append(neg_logits_with_rand_neg) max_logits1 = reduce_max(all_logits1) max_logits2 = reduce_max(all_logits2) final_logits = concat(max_logits1, max_logits2) loss = softmax_with_cross_entropy(final_logits) return loss def predict_interaction(q, d): avg_q = reduce_mean(q) return reduce_sum(avg_q * d) def train(query_tokens, pos_doc_tokens, neg_doc_tokens): query_embed = model.encode_query(query_tokens) pos_doc_embed = model_encode_doc(pos_doc_tokens) neg_doc_embed = model_encode_doc(neg_doc_tokens) loss = train_interaction(query_embed, pos_doc_embed, neg_doc_embed) apply_optimization(loss) def predict(query_tokens, doc_tokens): query_embed = model_encode_query(query_tokens) doc_embed = model_encode_doc(doc_tokens) score = predict_interaction(query_embed, doc_embed) return score</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Pseudo code of the model training and prediction.</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 https://github.com/PaddlePaddle/Paddle</figDesc></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Document Encoder (ERNIE) Doc Title Embedding Database Embedding ANN Indexes Offline Database &amp; Index Construction Text Matching Embedding ANN Term Indexes Embedding ANN Indexes</head><label></label><figDesc></figDesc><table><row><cell>Online ERNIE-Enhanced Retrieval</cell><cell cols="3">Post-Retrieval Filtering</cell><cell></cell></row><row><cell>! "</cell><cell># "</cell><cell>$ "</cell><cell>\u2026</cell><cell>%&amp;\'(% "</cell></row><row><cell>Conventional Workflow</cell><cell>Feature</cell><cell></cell><cell cols="2">Embedding</cell><cell>ERNIE-Enhanced</cell></row><row><cell></cell><cell>Database</cell><cell></cell><cell cols="2">Database</cell><cell>Workflow</cell></row><row><cell></cell><cell>Doc_ID</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Query Processing</cell><cell>Query Text</cell><cell></cell><cell>Query Encoder (ERNIE)</cell></row></table><note>\u2026 Doc_ID</note></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Offline experimental results on different stages.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Manual Auto-Rand Auto-Tail</cell></row><row><cell>Metric</cell><cell>PNR</cell><cell cols="2">Recall@10 Recall@10</cell></row><row><cell>Pretrain</cell><cell>1.34</cell><cell>18.92%</cell><cell>12.10%</cell></row><row><cell>Post-pretrain</cell><cell>1.43</cell><cell>35.99%</cell><cell>19.10%</cell></row><row><cell>Intermediate Fine-tune</cell><cell>2.00</cell><cell>83.14%</cell><cell>60.98%</cell></row><row><cell>Target Fine-tune</cell><cell>2.48</cell><cell>87.90%</cell><cell>71.13%</cell></row><row><cell>Online baseline</cell><cell>1.77</cell><cell>85.22%</cell><cell>65.79%</cell></row></table><note></note></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Interleaved comparison.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ENINE-based retrieval</cell><cell></cell><cell></cell><cell cols="2">Post-retreival filtering</cell><cell></cell></row><row><cell></cell><cell cols="2">Query type</cell><cell cols="2">Query length</cell><cell cols="2">Query type</cell><cell cols="2">Query length</cell></row><row><cell></cell><cell>Rand</cell><cell cols="3">Tail (freq &lt; 3) Short (\u226410) Long (10 &lt;)</cell><cell>Rand</cell><cell cols="3">Tail (freq &lt; 3) Short (\u226410) Long (10 &lt;)</cell></row><row><cell>\u0394</cell><cell>+0.368%</cell><cell>+0.992%</cell><cell>+0.345%</cell><cell>+0.426%</cell><cell>+0.112%</cell><cell>+0.274%</cell><cell>+0.066%</cell><cell>+0.191%</cell></row><row><cell cols="2">\u0394 -tw +0.281%</cell><cell>+0.783%</cell><cell>+0.253%</cell><cell>+0.352%</cell><cell>+0.226%</cell><cell>+0.454%</cell><cell>+0.176%</cell><cell>+0.315%</cell></row><row><cell></cell><cell></cell><cell cols="5">*All the values are statistically significant ( -test with &lt; 0.05).</cell><cell></cell><cell></cell></row></table><note></note></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Relative improvement on manual evaluation.</figDesc><table><row><cell></cell><cell cols="2">ERNIE-based retrieval</cell><cell cols="2">Post-retrieval filtering</cell></row><row><cell></cell><cell>Rand</cell><cell>Tail</cell><cell>Rand</cell><cell>Tail</cell></row><row><cell>\u0394</cell><cell>+0.17%</cell><cell>+0.22%</cell><cell>+0.10%</cell><cell>+0.65%</cell></row><row><cell>\u0394</cell><cell>+3.50%</cell><cell>+7.50%</cell><cell>+3.96%</cell><cell>+3.13%</cell></row><row><cell cols="5">*All the values are statistically significant ( -test with &lt; 0.05).</cell></row></table><note></note></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the improvements of our model.</figDesc><table><row><cell cols="2">ID Model</cell><cell cols="3">Search log Manual data Storage / doc</cell></row><row><cell>0</cell><cell>Base</cell><cell>2.15</cell><cell>1.76</cell><cell>3072 bytes</cell></row><row><cell>1</cell><cell>0 + Poly</cell><cell>2.23</cell><cell>1.81</cell><cell>3072 bytes</cell></row><row><cell>2</cell><cell>1 + IBN</cell><cell>2.33</cell><cell>2.06</cell><cell>3072 bytes</cell></row><row><cell>3</cell><cell>2 + Compression</cell><cell>2.26</cell><cell>2.00</cell><cell>1024 bytes</cell></row><row><cell>4</cell><cell>3 + Quantization</cell><cell>2.27</cell><cell>2.01</cell><cell>256 bytes</cell></row></table><note></note></figure><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>PNR values when varying the scoring method.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="2">Search log Manual data</cell></row><row><cell cols="3">max =1 \u2022 \u2032</cell><cell>2.205</cell><cell>1.955</cell></row><row><cell>1</cell><cell>=1</cell><cell>\u2022 \u2032</cell><cell>2.205</cell><cell>1.987</cell></row><row><cell>0 \u2022 \u2032 1 \u2022 \u2032 2 \u2022 \u2032 3 \u2022 \u2032</cell><cell></cell><cell></cell><cell>2.161 2.176 1.131 2.204</cell><cell>1.940 1.969 1.909 1.976</cell></row></table><note></note></figure></body><back><div type="references"><listBibl><biblStruct xml:id="b0"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiong</forename><surname>Cai</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqing</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName></author><idno type="arXiv">arXiv:2103.04831</idno><title level="m">Semantic Models for the First-stage Retrieval: A Comprehensive Review</title><imprint><date type="published" when="2021" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b1"><monogr><title level="m" type="main">Pre-training tasks for embedding-based large-scale retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Yu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Chang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Yang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName></author><idno type="arXiv">arXiv:2002.03932</idno><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b2"><analytic><title level="a" type="main">A comparative analysis of interleaving methods for aggregated search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Chuklin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Schuth</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName></author></analytic><monogr><title level="j">TOIS</title><imprint><biblScope unit="volume">33</biblScope><biblScope unit="page" from="1" to="38" /><date type="published" when="2015" /></imprint></monogr></biblStruct><biblStruct xml:id="b3"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName></author><idno type="arXiv">arXiv:1810.04805</idno><title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title><imprint><date type="published" when="2018" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b4"><analytic><title level="a" type="main">Triplet loss in siamese network for object tracking</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName></author></analytic><monogr><title level="m">ECCV</title><imprint><date type="published" when="2018" /><biblScope unit="page" from="459" to="474" /></imprint></monogr></biblStruct><biblStruct xml:id="b5"><monogr><title level="m" type="main">Zero-Resource Translation with Multi-Lingual Neural Machine Translation</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatos T Yarman</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Vural</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName></author><imprint><date type="published" when="2016" /><biblScope unit="page" from="268" to="277" /></imprint></monogr></biblStruct><biblStruct xml:id="b6"><monogr><title level="m" type="main">Modeling interestingness with deep neural networks</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName></author><imprint><date type="published" when="2014" /></imprint></monogr></biblStruct><biblStruct xml:id="b7"><analytic><title level="a" type="main">Clickthrough-based latent semantic models for web search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName></author></analytic><monogr><title level="m">SIGIR</title><imprint><date type="published" when="2011" /><biblScope unit="page" from="675" to="684" /></imprint></monogr></biblStruct><biblStruct xml:id="b8"><analytic><title level="a" type="main">Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Gu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2020" /><biblScope unit="page" from="2493" to="2500" /></imprint></monogr></biblStruct><biblStruct xml:id="b9"><analytic><title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2016" /><biblScope unit="page" from="55" to="64" /></imprint></monogr></biblStruct><biblStruct xml:id="b10"><monogr><title level="m" type="main">Improving Deep Learning For Airbnb Search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malay</forename><surname>Haldar</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Ramanathan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Sax</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Abdool</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamir</forename><surname>Mansawala</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Yang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Turnbull</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshuo</forename><surname>Liao</surname></persName></author><imprint><date type="published" when="2020" /><biblScope unit="page" from="2822" to="2830" /></imprint></monogr></biblStruct><biblStruct xml:id="b11"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName></author><idno type="arXiv">arXiv:1503.02531</idno><title level="m">Distilling the knowledge in a neural network</title><imprint><date type="published" when="2015" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b12"><monogr><title level="m" type="main">Embeddingbased retrieval in facebook search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sharma</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuying</forename><surname>Sun</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xia</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pronin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName></author><imprint><date type="published" when="2020" /><biblScope unit="page" from="2553" to="2561" /></imprint></monogr></biblStruct><biblStruct xml:id="b13"><analytic><title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2013" /><biblScope unit="page" from="2333" to="2338" /></imprint></monogr></biblStruct><biblStruct xml:id="b14"><analytic><title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName></author></analytic><monogr><title level="j">JMLR</title><imprint><biblScope unit="volume">18</biblScope><biblScope unit="page" from="6869" to="6898" /><date type="published" when="2017" /></imprint></monogr><note>Ran El-Yaniv, and Yoshua Bengio</note></biblStruct><biblStruct xml:id="b15"><monogr><title level="m" type="main">Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName></author><idno type="arXiv">arXiv:1905.01969</idno><imprint><date type="published" when="2019" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b16"><analytic><title level="a" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vi\xe9gas</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName></author></analytic><monogr><title level="j">TACL</title><imprint><biblScope unit="volume">5</biblScope><biblScope unit="page" from="339" to="351" /><date type="published" when="2017" /></imprint></monogr></biblStruct><biblStruct xml:id="b17"><analytic><title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName></author></analytic><monogr><title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP</title><meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP</meeting><imprint><date type="published" when="2020" /><biblScope unit="page" from="6769" to="6781" /></imprint></monogr></biblStruct><biblStruct xml:id="b18"><analytic><title level="a" type="main">Latent Retrieval for Weakly Supervised Open Domain Question Answering</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName></author></analytic><monogr><title level="m">ACL</title><imprint><date type="published" when="2019" /><biblScope unit="page" from="6086" to="6096" /></imprint></monogr></biblStruct><biblStruct xml:id="b19"><analytic><title level="a" type="main">Semantic matching in search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName></author></analytic><monogr><title level="j">Foundations and Trends in Information retrieval</title><imprint><biblScope unit="volume">7</biblScope><biblScope unit="page" from="343" to="469" /><date type="published" when="2014" /></imprint></monogr></biblStruct><biblStruct xml:id="b20"><analytic><title level="a" type="main">Decoupled Graph Convolution Network for Inferring Substitutable and Complementary Items</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Gu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchao</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Guo</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Yan</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2020" /><biblScope unit="page" from="2621" to="2628" /></imprint></monogr></biblStruct><biblStruct xml:id="b21"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName></author><idno type="arXiv">arXiv:1907.11692</idno><title level="m">Roberta: A robustly optimized bert pretraining approach</title><imprint><date type="published" when="2019" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b22"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmo</forename><surname>Ni</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName></author><idno type="arXiv">arXiv:2010.12523</idno><title level="m">Neural Passage Retrieval with Improved Negative Contrast</title><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b23"><analytic><title level="a" type="main">TwinBERT: Distilling Knowledge to Twin-Structured Compressed BERT Models for Large-Scale Retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Lu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2020" /><biblScope unit="page" from="2645" to="2652" /></imprint></monogr></biblStruct><biblStruct xml:id="b24"><monogr><title level="m" type="main">A deep architecture for matching short texts</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName></author><imprint><date type="published" when="2013" /><biblScope unit="volume">26</biblScope><biblScope unit="page" from="1367" to="1375" /></imprint></monogr></biblStruct><biblStruct xml:id="b25"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName></author><idno type="arXiv">arXiv:2005.00181</idno><title level="m">Sparse, dense, and attentional representations for text retrieval</title><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b26"><analytic><title level="a" type="main">An introduction to neural information retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Bhaskar Mitra</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craswell</surname></persName></author></analytic><monogr><title level="j">Now Foundations and Trends</title><imprint><date type="published" when="2018" /></imprint></monogr></biblStruct><biblStruct xml:id="b27"><analytic><title level="a" type="main">Learning to match using local and distributed representations of text for web search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName></author></analytic><monogr><title level="m">WWW</title><imprint><date type="published" when="2017" /><biblScope unit="page" from="1291" to="1299" /></imprint></monogr></biblStruct><biblStruct xml:id="b28"><monogr><title level="m" type="main">Semantic modelling with long-short-term memory for information retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Song</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName></author><idno type="arXiv">arXiv:1412.6629</idno><imprint><date type="published" when="2014" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b29"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palangi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName></author><title level="m">Deep Sentence Embedding Using the Long Short Term Memory Network: Analysis and Application to Information Retrieval. arXiv. org</title><imprint><date type="published" when="2015" /></imprint></monogr></biblStruct><biblStruct xml:id="b30"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName></author><idno type="arXiv">arXiv:1802.05365</idno><title level="m">Deep contextualized word representations</title><imprint><date type="published" when="2018" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b31"><monogr><title level="m" type="main">Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Phu Mon Htut</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Pang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Vania</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kann</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName></author><imprint><date type="published" when="2020" /><biblScope unit="page" from="5231" to="5247" /></imprint></monogr></biblStruct><biblStruct xml:id="b32"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName></author><title level="m">Semantic hashing. IJAR</title><imprint><date type="published" when="2009" /><biblScope unit="volume">50</biblScope><biblScope unit="page" from="969" to="978" /></imprint></monogr></biblStruct><biblStruct xml:id="b33"><monogr><title level="m" type="main">Introduction to information retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch\xfctze</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Manning</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan</surname></persName></author><imprint><date type="published" when="2008" /><publisher>Cambridge University Press</publisher><biblScope unit="volume">39</biblScope><pubPlace>Cambridge</pubPlace></imprint></monogr></biblStruct><biblStruct xml:id="b34"><analytic><title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName></author></analytic><monogr><title level="m">SIGIR</title><imprint><date type="published" when="2015" /><biblScope unit="page" from="373" to="382" /></imprint></monogr></biblStruct><biblStruct xml:id="b35"><analytic><title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr\xe9goire</forename><surname>Mesnil</surname></persName></author></analytic><monogr><title level="m">CIKM</title><imprint><date type="published" when="2014" /><biblScope unit="page" from="101" to="110" /></imprint></monogr></biblStruct><biblStruct xml:id="b36"><monogr><title level="m" type="main">Learning semantic representations using convolutional neural networks for web search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr\xe9goire</forename><surname>Mesnil</surname></persName></author><imprint><date type="published" when="2014" /><biblScope unit="page" from="373" to="374" /></imprint></monogr></biblStruct><biblStruct xml:id="b37"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName></author><idno type="arXiv">arXiv:1904.09223</idno><title level="m">Ernie: Enhanced representation through knowledge integration</title><imprint><date type="published" when="2019" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b38"><analytic><title level="a" type="main">Attention is all you need</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\u0141ukasz</forename><surname>Kaiser</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName></author></analytic><monogr><title level="m">NeurIPS</title><imprint><date type="published" when="2017" /><biblScope unit="page" from="5998" to="6008" /></imprint></monogr></biblStruct><biblStruct xml:id="b39"><analytic><title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName></author></analytic><monogr><title level="j">In AAAI</title><imprint><biblScope unit="volume">30</biblScope><date type="published" when="2016" /></imprint></monogr></biblStruct><biblStruct xml:id="b40"><analytic><title level="a" type="main">Modellevel dual learning</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName></author></analytic><monogr><title level="m">International Conference on Machine Learning. PMLR</title><imprint><date type="published" when="2018" /><biblScope unit="page" from="5383" to="5392" /></imprint></monogr></biblStruct><biblStruct xml:id="b41"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Xiong</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Li</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Ahmed</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Overwijk</surname></persName></author><idno type="arXiv">arXiv:2007.00808</idno><title level="m">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b42"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName></author><idno type="arXiv">arXiv:1906.08237</idno><title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title><imprint><date type="published" when="2019" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b43"><analytic><title level="a" type="main">Pretrained Transformers for Text Ranking: BERT and Beyond</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName></author></analytic><monogr><title level="m">WSDM</title><imprint><date type="published" when="2021" /><biblScope unit="page" from="1154" to="1156" /></imprint></monogr></biblStruct><biblStruct xml:id="b44"><analytic><title level="a" type="main">Learning discriminative projections for text similarity measures</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName></author></analytic><monogr><title level="m">CoNLL</title><imprint><date type="published" when="2011" /><biblScope unit="page" from="247" to="256" /></imprint></monogr></biblStruct><biblStruct xml:id="b45"><monogr><title level="m" type="main">Ranking relevance in yahoo search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Daly</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Ouyang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsung</forename><surname>Kang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Deng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikashi</forename><surname>Nobata</surname></persName></author><imprint><date type="published" when="2016" /><biblScope unit="page" from="323" to="332" /></imprint></monogr></biblStruct><biblStruct xml:id="b46"><monogr><title level="m" type="main">Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Wang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiling</forename><surname>Tang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjiang</forename><surname>Jiang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Yan</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yun</forename><surname>Yang</surname></persName></author><idno type="arXiv">arXiv:2006.02282</idno><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b47"><monogr><title/><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiji</forename><surname>Gao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Long</surname></persName></author><idno type="arXiv">arXiv:2006.14827</idno><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">2020. Memory-efficient Embedding for Recommendations. arXiv preprint</note></biblStruct><biblStruct xml:id="b48"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName></author><idno type="arXiv">arXiv:2002.11252</idno><title level="m">Xudong Zheng, Xiaobing Liu, and Jiliang Tang. 2020. Autoemb: Automated embedding dimensionality search in streaming recommendations</title><imprint><date type="published" when="2020" /></imprint></monogr><note type="report_type">arXiv preprint</note></biblStruct><biblStruct xml:id="b49"><monogr><title level="m" type="main">Reinforcement learning to optimize long-term user engagement in recommender systems</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Xia</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Song</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName></author><imprint><date type="published" when="2019" /><biblScope unit="page" from="2810" to="2818" /></imprint></monogr></biblStruct><biblStruct xml:id="b50"><analytic><title level="a" type="main">Pseudo Dyna-Q: A reinforcement learning framework for interactive recommendation</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Xia</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Du</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Bai</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Liu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName></author></analytic><monogr><title level="m">WSDM</title><imprint><date type="published" when="2020" /><biblScope unit="page" from="816" to="824" /></imprint></monogr></biblStruct><biblStruct xml:id="b51"><monogr><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Xia</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Gu</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Liu</surname></persName></author><title level="m">Jimmy Xiangji Huang, and Dawei Yin. 2020. Neural Interactive Collaborative Filtering. In SIGIR</title><imprint><biblScope unit="page" from="749" to="758" /></imprint></monogr></biblStruct><biblStruct xml:id="b52"><analytic><title level="a" type="main">Zhicong Cheng, and Dawei Yin. 2021. Pre-trained Language Model based Ranking in Baidu Search</title><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zou</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqiang</forename><surname>Zhang</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyi</forename><surname>Cai</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suqi</forename><surname>Cheng</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiting</forename><surname>Shi</surname></persName></author><author><persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName></author></analytic><monogr><title level="m">KDD</title><imprint/></monogr></biblStruct></listBibl></div></back></text></TEI>